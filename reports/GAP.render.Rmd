---
title: Generalized Assignment Problem
output:
  html_document:
    theme: cerulean
    toc: true
    toc_depth: 3
    toc_float: true
---

## About

This page contains an automatic benchmark between the C++ library [idol](https://github.com/hlefebvr/idol) and the Julia package [coluna.jl](https://github.com/atoptima/Coluna.jl)
to test their implementation of the [Branch-and-Price algorithm](https://en.wikipedia.org/wiki/Branch_and_price) for solving instances of the [Generalized Assignment Problem](https://en.wikipedia.org/wiki/Generalized_assignment_problem).

The results presented here are automatically generated using GitHub Actions and R with Rmarkdown. Note that the experiments themselves were run on GitHub Actions for which the code
is fully public and can be found [here for implementation details](https://github.com/hlefebvr/idol_benchmark) and [here for GitHub Actions configuration](https://github.com/hlefebvr/idol_benchmark/blob/main/.github/workflows/benchmark.yml).

The experiments were conducted on a GitHub-hosted runner with an `ubuntu-latest` virtual machine with two CPU cores (x86_64), 7 GB of RAM and 14 GB of SSD space (see [hardware specifications here](https://docs.github.com/en/actions/using-github-hosted-runners/about-github-hosted-runners)).

Last automatic run: `r format(Sys.time(), '%d/%m/%y %H:%M:%S')`.

```{r echo = FALSE}
library(rmarkdown)
library(tidyr)

knitr::opts_chunk$set(
  out.width = "100%"
)

```

## Mathematical models

### Direct model

Let $m$ be a given of agents and let $n$ be a set of tasks to perform. Let $c_{ij}$ be the cost of assigning task $j$ to agent $i$, $w_{ij}$ be the resource consumption of task $j$ when performed by agent $i$ and let $t_i$ be the resource capacity of agent $i$. The Generalized Assignment Problem (GAP) can be modeled as

$$
  \begin{array}{lll}
    \textrm{minimize } & \displaystyle \sum_{i=1}^m \sum_{j=1}^n c_{ij} x_{ij} \\
    \textrm{subject to } & \displaystyle \sum_{j=1}^n w_{ij}x_{ij} \le t_{i} & i=1,...,m \\
    & \displaystyle \sum_{i=1}^m x_{ij} = 1 & j = 1,...,n \\
    & x_{ij} \in \{ 0,1 \}.
  \end{array}
$$
Here, variable $x_{ij}$ encodes the assignment decision and equals $1$ if and only if task $j$ is assigned to agent $i$.

### Dantzig-Wolfe reformulation

Let us enumerate the list of all feasible assignments, i.e., let $\{ \bar{\mathrm{x}}_{ij}^e \}_{e\in E} = \{ x\in\{ 0, 1 \}^{mn} | \sum_{j=1}^n w_{ij}x_{ij} \le t_i \quad i = 1,...,m \}$ where $E$ denotes a list for their indices. The [Dantzig-Wolfe reformulation](https://en.wikipedia.org/wiki/Dantzig%E2%80%93Wolfe_decomposition) of GAP reads

$$
  \begin{array}{lll}
    \textrm{minimize } & \displaystyle \sum_{e\in E} \alpha_e \left( \sum_{i=1}^m\sum_{j=1}^n c_{ij}\bar{\mathrm{x}}_{ij}^e \right) \\
    \textrm{subject to } & \displaystyle \sum_{i=1}^m \alpha_e\bar{\mathrm{x}}_{ij}^e = 1 & j=1,...,n \\
    & \displaystyle \sum_{e\in E} \alpha_e = 1 \\
    & \alpha_e \ge 0
  \end{array}
$$

Though this model contains an exponential number of variables (i.e., columns) it can still be solved efficiently using [Column Generation](https://en.wikipedia.org/wiki/Column_generation) and [Branch and price](https://en.wikipedia.org/wiki/Branch_and_price). In such case, the pricing problem is a [Knapsack Problem](https://en.wikipedia.org/wiki/Knapsack_problem).

## Reading instances

In this section, we start by reading the raw computational results stored in CSV files. Note that these CSV files is stored as an *artifact* on the [GitHub Actions of the hlefebvr/idol_benchmark repository](https://github.com/hlefebvr/idol_benchmark/actions) and can be downloaded without restrictions by clicking on the latest workflow execution named `new_workflow` under the section "Artifacts" (also note that artifacts have a life span of 90 days).


```{r}
time_limit = 10 * 60
```


### Results obtained with Idol

We first start to read results obtained using idol, which can be found inside the `results_GAP_idol.csv` file (Note that this file can be obtained by running `cat results_GAP_idol__*.csv > results_GAP_idol.csv` after having extracted the `.zip` the artifact).

```{r}
idol = read.csv("../results_GAP_idol.csv", header = FALSE)
colnames(idol) = c("instance", "solver", "with_heuristics", "smoothing_factor", "farkas_pricing", "clean_up_threshold", "branching_on_master", "n_agents", "n_jobs", "status", "reason", "objective_value", "time")
```

Since the "external_solver" approach (as well as the "idol_bab" approach) do not have relevant parameter values for "with_heuristics", "smoothing_factor", "farkas_pricing", "clean_up_threshold" and "branching_on_master", we set these values to `NA`.

```{r}
idol[idol$solver == "external_solver",]$with_heuristics = NA
idol[idol$solver == "external_solver" | idol$solver == "idol_bab",]$smoothing_factor = NA
idol[idol$solver == "external_solver" | idol$solver == "idol_bab",]$farkas_pricing = NA
idol[idol$solver == "external_solver" | idol$solver == "idol_bab",]$clean_up_threshold = NA
idol[idol$solver == "external_solver" | idol$solver == "idol_bab",]$branching_on_master = NA
```

Then, we concatenate the parameter values with the method's name to obtain a unique identifier for each tested method. The unique identifier can be `external_solver`, `idol_bab_h<A>` or `idol_bap_h<A>_s<B>_f<C>_c<D>_m<E>` where

- `<A>` is 1 if the algorithm uses primal heuristics, 0 otherwise ; 
- `<B>` is the smoothing factor (when relevant) multiplied by 100 ;
- `<C>` is 1 if Farkas pricing was used, 0 if artificial variables were used ;
- `<D>` is the clean up threshold which was used (when the threshold is reached, up to 1/3 of the threshold columns are removed from the pool) ;
- `<E>` is 1 branching was applied to the master problem and 0 otherwise.

```{r}
idol$solver_with_params = paste0(idol$solver, "_h", idol$with_heuristics, "_s", idol$smoothing_factor * 100, "_f", idol$farkas_pricing, "_c", idol$clean_up, "_m", idol$branching_on_master)
idol[idol$solver == "external_solver",]$solver_with_params = "external_solver"
idol[idol$solver == "idol_bab",]$solver_with_params = paste0("idol_bab_h", idol[idol$solver == "idol_bab",]$with_heuristics)
```

### Results obtained with coluna.jl

The results obtained using coluna can be found inside `results_GAP_coluna.csv`.

```{r}
coluna =  read.csv("../results_GAP_coluna.csv", header = FALSE)
colnames(coluna) = c("instance", "solver", "with_heuristics", "smoothing_factor", "farkas_pricing", "clean_up_threshold", "branching_on_master", "status", "reason", "n_agents", "n_jobs", "objective_value", "time")
coluna$solver_with_params = "coluna"

coluna[coluna$status == "ERROR",]$time = time_limit
```

### All

We can now combine and print out the obtained computational results for each instance for each solver.

```{r}
results = rbind(idol, coluna)
```


```{r echo = FALSE}
paged_table(results)
```

The list of all tested configuration of solvers can be obtained as follows.

```{r}
solvers = as.data.frame(unique(results$solver_with_params))
solvers = as.data.frame(sort(solvers[,1], decreasing = TRUE))
colnames(solvers) = "Solver"
```

```{r echo = FALSE}
knitr::kable(solvers)
```

```{r}
colors = cbind(solvers, rainbow(nrow(solvers)))
```

## Checking results

In this section, we first make sure that every solver reports the same optimal objective value for solved instances. We thus introduce function `compare_results` which takes as input a set of results and two solver names for which the results should be compared. The function returns the list of instances which for which the two methods report different objective values (with a tolerance of $10^{-3}$).

```{r}
solved_results = results[results$time < time_limit,]

compare_results = function (dataset, solver_a, solver_b) {
  
  results_a = dataset[dataset$solver_with_params == solver_a & dataset$time < time_limit,]
  results_b = dataset[dataset$solver_with_params == solver_b & dataset$time < time_limit,]
  
  merged = merge(results_a, results_b, by = c("instance"))
  
  return ( merged[ abs(merged$objective_value.x - merged$objective_value.y) > 1e-2 ,] )
}
```

Then, we compare all solvers together.

```{r}
for (solver_with_params in unique(results$solver_with_params)) {
  mismatch = compare_results(results, "external_solver", solver_with_params)
  if (nrow(mismatch) > 0) {
    paged_table(mismatch)
  }
}
```

## Some helper functions

### Performance profiles

To compare the different methods, we use performance profiles as introduced in [Dolan et al. (2002)](https://link.springer.com/article/10.1007/s101070100263). The following function is used to compute and plot the performance profile of a given dataset.

```{r}
performance_profile = function (dataset, xlim = NULL, main = "Performance profile") {
  
  solvers = unique(dataset$solver_with_params)
  times = spread(dataset[,c("instance", "solver_with_params", "time")], key = solver_with_params, value = time)
  #times = na.omit(times)
  times$time.best = apply(times[,-1], 1, FUN = min)
  
  ratios = times[,-ncol(times)][,-1] / times$time.best
  colnames(ratios) = paste0(colnames(ratios), ".ratio")
  
  worst_ratio = max(ratios)
  
  times = cbind(times, ratios)
  
  for (solver in solvers) {
    time_limit_filter = times[,solver] >= time_limit
    if ( sum(time_limit_filter) > 0 ) {
      times[time_limit_filter, paste0(solver, ".ratio")] = worst_ratio
    }
  }
  
  if (is.null(xlim)) {
    xlim = c(1, worst_ratio)
  }
  
  par(mar = c(5,4,4,8))
  
  using_colors = NULL
  
  index = 1
  for (solver in solvers) {
    
    plot_function = if (index == 1) plot else lines
    
    profile = ecdf(times[,paste0(solver, ".ratio")])
    
    using_color =  colors[colors$Solver == solver,2]
    using_colors = rbind(using_colors, using_color)
    
    plot_function(profile, xlim = xlim, ylim = c(0,1), lty = "solid", cex = 0, col = using_color, main = "", xlab = "", ylab = "")
    
    index = index + 1
  }
  
  # Set the plot title
  title(main = main,
        xlab = "Performance ratio",
        ylab = "ECDF")
  
  # Set the plot legend
  legend(
    "topright",
    inset=c(-.35, 0),
    legend = solvers,
    lty = "solid",
    col = using_colors,
    cex = .5,
    xpd = TRUE,
    bty = "n"
  )
  
}
```

For instance, the following executation will plot a performance profile over all tested methods.

```{r}
performance_profile(results, xlim = c(1, 200))
```

Similarly, the following will compare specific methods together (in particular, GLPK, idol's branch-and-bound, idol's branch-price-price with primal heuristics, artificial variables, branching on master with smoothing factor 0.3 and coluna).

```{r}
performance_profile(results[results$solver_with_params %in% c("external_solver", "idol_bab_h1", "idol_bap_h1_s0_f0_c1500_m1", "coluna"),], xlim = c(1, 200), main = "External solver, Coluna, BaB and BaP")
```

### Summary tables

We also introduce function `make_table` which builds a summary table from a given dataset where each column is defined as:

- `solver_with_params` the given name of the method ;
- `n_agents` the number of agents of the considered instances ;
- `n_jobs` the number of jobs of the considered instances ;
- `count` the total number of instances which were tried ;
- `unsolved` the number of instances which could not be solved within the given time limit (i.e., within `r time_limit` seconds) ; 
- `time` the average computation time (over the solved instances only).

```{r}
make_table = function (dataset) {
  
  group_by = c("solver_with_params", "n_agents", "n_jobs")
  
  rownames(dataset) = NULL
  
  count = aggregate(dataset$time, by = dataset[,group_by], FUN = length)
  colnames(count) = c(group_by, "count")
  
  unsolved = aggregate(dataset$time >= time_limit, by = dataset[,group_by], FUN = sum)
  colnames(unsolved) = c(group_by, "unsolved")
  
  errors = aggregate(dataset$status == "ERROR", by = dataset[,group_by], FUN = sum)
  colnames(errors) = c(group_by, "error")
  
  solved_filter = dataset$time < time_limit
  times = aggregate(dataset[solved_filter,]$time, by = dataset[solved_filter,group_by], FUN = mean)
  colnames(times) = c(group_by, "time")
  
  Table = merge(count, unsolved, by = group_by)
  Table = merge(Table, errors, by = group_by)
  Table = merge(Table, times, by = group_by)

  return (Table)
}
```

For example, the following table gathers results from the Branch-and-Bound algorithm of idol.

```{r}
Table = make_table(results[results$solver == "idol_bab",])
```

```{r echo = FALSE}
knitr::kable(Table)
```

## Computational results

### Branch-and-bound algorithm

In this section, we evaluate the computational benefits obtained by idol's primal heuristics.

```{r}
performance_profile(results[results$solver == "idol_bab",], xlim = c(1, 10), main = "With and without heuristics")
performance_profile(results[results$solver == "idol_bab",], xlim = c(1, 200), main = "With and without heuristics")
```

### Branch and price

#### With coluna

```{r}
performance_profile(results[results$solver != "external_solver" & results$solver != "idol_bab",], xlim = c(1, 10), main = "idol versus coluna (up to 10)")
performance_profile(results[results$solver != "external_solver" & results$solver != "idol_bab",], xlim = c(1, 200), main = "idol versus coluna (up to 200)")
```


```{r}
Table = make_table(results[results$solver_with_params == "idol_bap_h1_s0_f0_c1500_m1" | results$solver_with_params == "coluna",])
```

```{r echo = FALSE}
knitr::kable(Table)
```

We also have a look at pathological instances for idol where the execution time is $20%$ worst than coluna's time. This is to spot and study why this happens and potentially fix the issue.

```{r}
find_pathological_instances = function (dataset, solver_a, solver_b) {
  
  results_a = dataset[dataset$solver_with_params == solver_a,]
  results_b = dataset[dataset$solver_with_params == solver_b,]
  
  merged = merge(results_a, results_b, by = c("instance"))
  
  result = merged[ merged$time.y > 1.2 * merged$time.x , c("instance", "time.x", "time.y", "status.x", "status.y")]
  
  colnames(result) = c("instance", paste0(solver_a, ".time"), paste0(solver_b, ".time"), paste0(solver_a, ".status"), paste0(solver_b, ".status"))
  
  return ( result )
}

pathological_instances = find_pathological_instances(results, "coluna", "idol_bap_h1_s0_f0_c1500_m1")
```

```{r}
paged_table(pathological_instances)
```

#### Branching on master versus branching on pricing

Comparing branching scheme (on master or on pricing problem) without smoothing, using artificial variables (phase I) and primal heuristics.

```{r}
performance_profile(results[results$solver == "idol_bap" & results$with_heuristics == TRUE & results$smoothing_factor == 0 & results$farkas_pricing == 0,], xlim = c(1, 10), main = "Branching on master versus subproblem (up to 10)")
performance_profile(results[results$solver == "idol_bap" & results$with_heuristics == TRUE & results$smoothing_factor == 0 & results$farkas_pricing == 0,], xlim = c(1, 200), main = "Branching on master versus subproblem (up to 200)")
```

#### Using artificial variables versus Farkas pricing

Comparing farkas pricing with artificial variables (phase I) using primal heuristics, master branching and no smoothing.

```{r}
performance_profile(results[results$solver == "idol_bap" & results$with_heuristics == TRUE & results$smoothing_factor == 0 & results$branching_on_master == TRUE,], xlim = c(1, 10), main = "Using artificial costs versus Farkas pricing (up to 10)")
performance_profile(results[results$solver == "idol_bap" & results$with_heuristics == TRUE & results$smoothing_factor == 0 & results$branching_on_master == TRUE,], xlim = c(1, 200), main = "Using artificial costs versus Farkas pricing (up to 200)")
```

#### Smoothing factor

Comparing smoothing factors with artificial variables, master branching and primal heuristics.

```{r}
performance_profile(results[results$solver == "idol_bap" & results$with_heuristics == TRUE & results$branching_on_master == TRUE & results$farkas_pricing == 0,], xlim = c(1, 10), main = "Smoothing factor (up to 10)")
performance_profile(results[results$solver == "idol_bap" & results$with_heuristics == TRUE & results$branching_on_master == TRUE & results$farkas_pricing == 0,], xlim = c(1, 200), main = "Smoothing factor (up to 200)")
```
---
title: Generalized Assignment Problem
output:
  rmdformats::readthedown:
    toc: 3
---

## About

This page contains an automatic benchmark between the C++ library [idol](https://github.com/hlefebvr/idol) and the Julia package [Coluna.jl](https://github.com/atoptima/Coluna.jl)
to test their implementation of the [Branch-and-Price algorithm](https://en.wikipedia.org/wiki/Branch_and_price) for solving instances of the [Generalized Assignment Problem](https://en.wikipedia.org/wiki/Generalized_assignment_problem).

The results presented here are automatically generated using GitHub Actions and R with Rmarkdown. Note that the experiments themselves are run with GitHub Actions for which the code
is fully public and can be found [here for implementation details](https://github.com/hlefebvr/idol_benchmark) and [here for GitHub Actions configuration](https://github.com/hlefebvr/idol_benchmark/blob/main/.github/workflows/benchmark.yml).

The experiments were conducted on a free GitHub-hosted runner with an `ubuntu-latest` virtual machine with two CPU cores (x86_64), 7 GB of RAM and 14 GB of SSD space (see [hardware specifications here](https://docs.github.com/en/actions/using-github-hosted-runners/about-github-hosted-runners)).

Last automatic run: `r format(Sys.time(), '%d/%m/%y %H:%M:%S')`.

```{r echo = FALSE}
library(rmarkdown)
library(tidyr)
library(ggplot2)

knitr::opts_chunk$set(
  out.width = "100%"
)

```

## Mathematical models

### Direct model

Let $m$ be a given of agents and let $n$ be a set of tasks to perform. Let $c_{ij}$ be the cost of assigning task $j$ to agent $i$, $w_{ij}$ be the resource consumption of task $j$ when performed by agent $i$ and let $t_i$ be the resource capacity of agent $i$. The Generalized Assignment Problem (GAP) can be modeled as

$$
  \begin{array}{lll}
    \textrm{minimize } & \displaystyle \sum_{i=1}^m \sum_{j=1}^n c_{ij} x_{ij} \\
    \textrm{subject to } & \displaystyle \sum_{j=1}^n w_{ij}x_{ij} \le t_{i} & i=1,...,m \\
    & \displaystyle \sum_{i=1}^m x_{ij} = 1 & j = 1,...,n \\
    & x_{ij} \in \{ 0,1 \}.
  \end{array}
$$
Here, variable $x_{ij}$ encodes the assignment decision and equals $1$ if and only if task $j$ is assigned to agent $i$.

### Dantzig-Wolfe reformulation

Let us enumerate the list of all feasible assignments, i.e., let $\{ \bar{\mathrm{x}}_{ij}^e \}_{e\in E} = \{ x\in\{ 0, 1 \}^{mn} | \sum_{j=1}^n w_{ij}x_{ij} \le t_i \quad i = 1,...,m \}$ where $E$ denotes a list for their indices. The [Dantzig-Wolfe reformulation](https://en.wikipedia.org/wiki/Dantzig%E2%80%93Wolfe_decomposition) of GAP reads

$$
  \begin{array}{lll}
    \textrm{minimize } & \displaystyle \sum_{e\in E} \alpha_e \left( \sum_{i=1}^m\sum_{j=1}^n c_{ij}\bar{\mathrm{x}}_{ij}^e \right) \\
    \textrm{subject to } & \displaystyle \sum_{i=1}^m \alpha_e\bar{\mathrm{x}}_{ij}^e = 1 & j=1,...,n \\
    & \displaystyle \sum_{e\in E} \alpha_e = 1 \\
    & \alpha_e \ge 0
  \end{array}
$$

Though this model contains an exponential number of variables (i.e., columns) it can still be solved efficiently using [Column Generation](https://en.wikipedia.org/wiki/Column_generation) and [Branch and price](https://en.wikipedia.org/wiki/Branch_and_price). In such case, the pricing problem is a [Knapsack Problem](https://en.wikipedia.org/wiki/Knapsack_problem).

## Reading instances

In this section, we start by reading the raw computational results stored in CSV files. Note that these CSV files is stored as an *artifact* on the [GitHub Actions of the hlefebvr/idol_benchmark repository](https://github.com/hlefebvr/idol_benchmark/actions) and can be downloaded without restrictions by clicking on the latest workflow execution named `new_workflow` under the section "Artifacts" (also note that artifacts have a life span of 90 days).


```{r}
time_limit = 5 * 60
```


### Results obtained with idol

We first start to read results obtained using idol, which can be found inside the `results_GAP_idol.csv` file (Note that this file can be obtained by running `cat results_GAP_idol__*.csv > results_GAP_idol.csv` after having extracted the `.zip` the artifact).

```{r}
idol = read.csv("../results_GAP_idol.csv", header = FALSE)
colnames(idol) = c("instance", "solver", "with_heuristics", "smoothing_factor", "farkas_pricing", "clean_up_threshold", "branching_on_master", "n_agents", "n_jobs", "status", "reason", "objective_value", "time")
```

Since the "external" approach (as well as the "idol_bab" approach) do not have relevant parameter values for "with_heuristics", "smoothing_factor", "farkas_pricing", "clean_up_threshold" and "branching_on_master", we set these values to `NA`.

```{r}
idol[idol$solver == "external",]$with_heuristics = NA
idol[idol$solver == "external" | idol$solver == "idol_bab",]$smoothing_factor = NA
idol[idol$solver == "external" | idol$solver == "idol_bab",]$farkas_pricing = NA
idol[idol$solver == "external" | idol$solver == "idol_bab",]$clean_up_threshold = NA
idol[idol$solver == "external" | idol$solver == "idol_bab",]$branching_on_master = NA
```

Then, we concatenate the parameter values with the method's name to obtain a unique identifier for each tested method. The unique identifier can be `external`, `idol_bab_h<A>` or `idol_bap_h<A>_s<B>_f<C>_c<D>_m<E>` where

- `<A>` is 1 if the algorithm uses primal heuristics, 0 otherwise ; 
- `<B>` is the smoothing factor (when relevant) multiplied by 100 ;
- `<C>` is 1 if Farkas pricing was used, 0 if artificial variables were used ;
- `<D>` is the clean up threshold which was used (when the threshold is reached, up to 1/3 of the threshold columns are removed from the pool) ;
- `<E>` is 1 branching was applied to the master problem and 0 otherwise.

```{r}
idol$solver_with_params = paste0(idol$solver, "_h", idol$with_heuristics, "_s", idol$smoothing_factor * 100, "_f", idol$farkas_pricing, "_c", idol$clean_up, "_m", idol$branching_on_master)
idol[idol$solver == "external",]$solver_with_params = "external"
idol[idol$solver == "idol_bab",]$solver_with_params = paste0("idol_bab_h", idol[idol$solver == "idol_bab",]$with_heuristics)
```

### Results obtained with coluna.jl

The results obtained using coluna can be found inside `results_GAP_coluna.csv`.

```{r}
coluna =  read.csv("../results_GAP_coluna.csv", header = FALSE)
colnames(coluna) = c("instance", "solver", "with_heuristics", "smoothing_factor", "farkas_pricing", "clean_up_threshold", "branching_on_master", "status", "reason", "n_agents", "n_jobs", "objective_value", "time")
coluna$solver_with_params = "coluna"

if (sum(coluna$status == "ERROR") > 0) {
  coluna[coluna$status == "ERROR",]$time = time_limit 
}

if (sum(coluna$status == "TIME_LIMIT") > 0) {
  coluna[coluna$status == "TIME_LIMIT",]$time = time_limit 
}
```

### All

We can now combine and print out the obtained computational results for each instance for each solver.

```{r}
results = rbind(idol, coluna)
```


```{r echo = FALSE}
paged_table(results)
```

The list of all tested configuration of solvers can be obtained as follows.

```{r}
solvers = as.data.frame(unique(results$solver_with_params))
solvers = as.data.frame(sort(solvers[,1], decreasing = TRUE))
colnames(solvers) = "Solver"
```

```{r echo = FALSE}
knitr::kable(solvers)
```

## Checking results

In this section, we first make sure that every solver reports the same optimal objective value for solved instances. We thus introduce function `compare_results` which takes as input a set of results and two solver names for which the results should be compared. The function returns the list of instances which for which the two methods report different objective values (with a tolerance of $10^{-3}$).

```{r}
compare_results = function (dataset, solver_a, solver_b) {
  
  results_a = dataset[dataset$solver_with_params == solver_a & dataset$time < time_limit,]
  results_b = dataset[dataset$solver_with_params == solver_b & dataset$time < time_limit,]
  
  merged = merge(results_a, results_b, by = c("instance"))
  
  return ( merged[ abs(merged$objective_value.x - merged$objective_value.y) > 1e-2 ,] )
}
```

Then, we compare all solvers together.

```{r}
for (solver_with_params in unique(results$solver_with_params)) {
  mismatch = compare_results(results, "external", solver_with_params)
  if (nrow(mismatch) > 0) {
    paged_table(mismatch)
  }
}
```

## Some helper functions

### ECDF

To compare the different methods, we use ECDF of computation times. The following function is used to plot the ECDF of computation time of a given data set.
Each solver keeping the same color in the whole document.

```{r}
# Define color palette
colors <- scales::hue_pal()(length(unique(results$solver_with_params)))

# Define a mapping solver_with_params[i] -> color[i]
color_mapping = setNames(colors[1:length(solvers$Solver)], solvers$Solver)
  
# Define function plot_ecdf
plot_ecdf = function (dataset, main = "ECDF", log = FALSE) {
  
  result = ggplot(dataset, aes(x = time, color = solver_with_params)) +
    stat_ecdf(geom = "step") +
    xlab("Total Time") +
    ylab("ECDF") +
    labs(title = main)  +
    scale_color_manual(values = color_mapping) +
    theme_minimal()
  
  if (log) {
    result = result + scale_x_log10()
  }
  
  result
  
}
```

For instance, the following execution will plot the ECDF over all tested methods.

```{r}
plot_ecdf(results)
```

### Summary tables

We also introduce function `make_table` which builds a summary table from a given dataset where each column is defined as:

- `solver_with_params` the given name of the method ;
- `n_agents` the number of agents of the considered instances ;
- `n_jobs` the number of jobs of the considered instances ;
- `count` the total number of instances which were tried ;
- `unsolved` the number of instances which could not be solved within the given time limit (i.e., within `r time_limit` seconds) ; 
- `time` the average computation time (over the solved instances only).

```{r}
make_table = function (dataset) {
  
  group_by = c("solver_with_params", "n_agents", "n_jobs")
  
  rownames(dataset) = NULL
  
  count = aggregate(dataset$time, by = dataset[,group_by], FUN = length)
  colnames(count) = c(group_by, "count")
  
  unsolved = aggregate(dataset$time >= time_limit & dataset$status != "ERROR", by = dataset[,group_by], FUN = sum)
  colnames(unsolved) = c(group_by, "unsolved")
  
  errors = aggregate(dataset$status == "ERROR", by = dataset[,group_by], FUN = sum)
  colnames(errors) = c(group_by, "error")
  
  solved_filter = dataset$time < time_limit
  times = aggregate(dataset[solved_filter,]$time, by = dataset[solved_filter,group_by], FUN = mean)
  colnames(times) = c(group_by, "time")
  
  Table = merge(count, unsolved, by = group_by)
  Table = merge(Table, errors, by = group_by)
  Table = merge(Table, times, by = group_by)

  return (Table)
}
```

For example, the following table gathers results from the external solver.

```{r}
Table = make_table(results[results$solver == "external",])
```

```{r echo = FALSE}
knitr::kable(Table)
```

## Computational results

### Branch-and-bound algorithm with idol

In this section, we evaluate the computational benefits obtained by idol's primal heuristics.

```{r}
plot_ecdf(
  results[results$solver == "idol_bab",], 
  main = "Branch-and-bound: with v.s. without heuristics"
)
```

```{r}
TableBaB = make_table(results[results$solver == "idol_bab",])
```

```{r echo = FALSE}
knitr::kable(TableBaB)
```

### Branch-and-price

#### Comparing with Coluna.jl

```{r}
plot_ecdf(
  results[results$solver != "external" & results$solver != "idol_bab",],
  main = "idol versus Coluna.jl",
  log = TRUE
)
```


```{r}
Table = make_table(
  results[results$solver_with_params != "external" & results$solver_with_params != "idol_bab_h0" & results$solver_with_params != "idol_bab_h1",]
)
```

```{r echo = FALSE}
knitr::kable(Table)
```

We also have a look at pathological instances for idol where the execution time is 20 \% worst than Coluna.jl's time. This is to spot and study why this happens and potentially fix the issue.

```{r}
find_pathological_instances = function (dataset, solver_a, solver_b) {
  
  results_a = dataset[dataset$solver_with_params == solver_a,]
  results_b = dataset[dataset$solver_with_params == solver_b,]
  
  merged = merge(results_a, results_b, by = c("instance"))
  
  result = merged[ merged$time.y > 1.2 * merged$time.x , c("instance", "time.x", "time.y", "status.x", "status.y")]
  
  colnames(result) = c("instance", paste0(solver_a, ".time"), paste0(solver_b, ".time"), paste0(solver_a, ".status"), paste0(solver_b, ".status"))
  
  rownames(result) = NULL
  
  return ( result )
}

pathological_instances = find_pathological_instances(results, "coluna", "idol_bap_h1_s0_f0_c1500_m1")
```

```{r}
paged_table(pathological_instances)
```

#### Comparing different settings of idol

##### All different settings of idol

```{r}
plot_ecdf(results[results$solver == "idol_bap",], main = "idol", log = TRUE)
```

##### Branching strategy: on master v.s. on pricing

Comparing branching scheme (on master or on pricing problem) without smoothing, using artificial variables (phase I) and primal heuristics.

```{r}
plot_ecdf(
  results[results$solver == "idol_bap" & results$with_heuristics == TRUE & results$smoothing_factor == 0 & results$farkas_pricing == 0,], 
  main = "Branching strategy: on master v.s. on pricing", 
  log = TRUE
)
```

##### Handling infeasibility: slack variables v.s. Farkas pricing

Comparing Farkas pricing with artificial variables (phase I) using primal heuristics, master branching and no smoothing.

```{r}
plot_ecdf(
  results[results$solver == "idol_bap" & results$with_heuristics == TRUE & results$smoothing_factor == 0 & results$branching_on_master == TRUE,],
  main = "Handling Infeasibiliy: slack variables v.s. Farkas pricing", 
  log = TRUE
)
```

##### Stabilization by smoothing dual values

Comparing smoothing factors with artificial variables, master branching and primal heuristics.

```{r}
plot_ecdf(
  results[results$solver == "idol_bap" & results$with_heuristics == TRUE & results$branching_on_master == TRUE & results$farkas_pricing == TRUE,], 
  main = expression("Stabilization by smoothing dual values: different values for alpha"), 
  log = TRUE
)
```
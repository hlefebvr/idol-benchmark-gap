---
title: Generalized Assignment Problem
---

## About

This page contains an automatic benchmark between the C++ library [idol](https://github.com/hlefebvr/idol) and the Julia package [coluna.jl](https://github.com/atoptima/Coluna.jl)
to test their implementation of the [Branch-and-Price algorithm](https://en.wikipedia.org/wiki/Branch_and_price) for solving instances of the [Generalized Assignment Problem](https://en.wikipedia.org/wiki/Generalized_assignment_problem).

The results presented here are automatically generated using GitHub Actions and R with Rmarkdown. Note that the experiments themselves were run on GitHub Actions for which the code
is fully public and can be found [here for implementation details](https://github.com/hlefebvr/idol_benchmark) and [here for GitHub Actions configuration](https://github.com/hlefebvr/idol_benchmark/blob/main/.github/workflows/new_workflow.yml).

The experiments were conducted on a GitHub-hosted runner with an `ubuntu-latest` virtual machine with two CPU cores (x86_64), 7 GB of RAM and 14 GB of SSD space (see [hardware specifications here](https://docs.github.com/en/actions/using-github-hosted-runners/about-github-hosted-runners)).

Last automatic run: `r format(Sys.time(), '%d/%m/%y %H:%M:%S')`.

```{r echo = FALSE}
library(rmarkdown)
library(tidyr)

knitr::opts_chunk$set(
  out.width = "100%"
)

```

## Mathematical models

### Direct model

Let $m$ be a given of agents and let $n$ be a set of tasks to perform. Let $c_{ij}$ be the cost of assigning task $j$ to agent $i$, $w_{ij}$ be the resource consumption of task $j$ when performed by agent $i$ and let $t_i$ be the resource capacity of agent $i$. The Generalized Assignment Problem (GAP) can be modeled as

$$
  \begin{array}{lll}
    \textrm{minimize } & \displaystyle \sum_{i=1}^m \sum_{j=1}^n c_{ij} x_{ij} \\
    \textrm{subject to } & \displaystyle \sum_{j=1}^n w_{ij}x_{ij} \le t_{i} & i=1,...,m \\
    & \displaystyle \sum_{i=1}^m x_{ij} = 1 & j = 1,...,n \\
    & x_{ij} \in \{ 0,1 \}.
  \end{array}
$$
Here, variable $x_{ij}$ encodes the assignment decision and equals $1$ if and only if task $j$ is assigned to agent $i$.

### Dantzig-Wolfe reformulation

Let us enumerate the list of all feasible assignments, i.e., let $\{ \bar{\mathrm{x}}_{ij}^e \}_{e\in E} = \{ x\in\{ 0, 1 \}^{mn} | \sum_{j=1}^n w_{ij}x_{ij} \le t_i \quad i = 1,...,m \}$ where $E$ denotes a list for their indices. The [Dantzig-Wolfe reformulation](https://en.wikipedia.org/wiki/Dantzig%E2%80%93Wolfe_decomposition) of GAP reads

$$
  \begin{array}{lll}
    \textrm{minimize } & \displaystyle \sum_{e\in E} \alpha_e \left( \sum_{i=1}^m\sum_{j=1}^n c_{ij}\bar{\mathrm{x}}_{ij}^e \right) \\
    \textrm{subject to } & \displaystyle \sum_{i=1}^m \alpha_e\bar{\mathrm{x}}_{ij}^e = 1 & j=1,...,n \\
    & \displaystyle \sum_{e\in E} \alpha_e = 1 \\
    & \alpha_e \ge 0
  \end{array}
$$

Though this model contains an exponential number of variables (i.e., columns) it can still be solved efficiently using [Column Generation](https://en.wikipedia.org/wiki/Column_generation) and [Branch and price](https://en.wikipedia.org/wiki/Branch_and_price). In such case, the pricing problem is a [Knapsack Problem](https://en.wikipedia.org/wiki/Knapsack_problem).

## Reading instances

In this section, we start by reading the raw computational results stored in CSV files. Note that these CSV files is stored as an *artifact* on the [GitHub Actions of the hlefebvr/idol_benchmark repository](https://github.com/hlefebvr/idol_benchmark/actions) and can be downloaded without restrictions by clicking on the latest workflow execution named `new_workflow` under the section "Artifacts" (also note that artifacts have a life span of 90 days).

### Results obtained with Idol

We first start to read results obtained using idol, which can be found inside the `results_GAP_idol.csv` file (Note that this file can be obtained by running `cat results_GAP_idol__*.csv > results_GAP_idol.csv` after having extracted the `.zip` the artifact).

```{r}
idol = read.csv("../results_GAP_idol.csv", header = FALSE)
colnames(idol) = c("instance", "solver", "with_heuristics", "smoothing_factor", "farkas_pricing", "clean_up_threshold", "branching_on_master", "status", "reason", "n_agents", "n_jobs", "objective_value", "time")
```

Since the "external_solver" approach (as well as the "idol_bab" approach) do not have relevant parameter values for "with_heuristics", "smoothing_factor", "farkas_pricing", "clean_up_threshold" and "branching_on_master", we set these values to `NA`.

```{r}
idol[idol$solver == "external_solver",]$with_heuristics = NA
idol[idol$solver == "external_solver" | idol$solver == "idol_bab",]$smoothing_factor = NA
idol[idol$solver == "external_solver" | idol$solver == "idol_bab",]$farkas_pricing = NA
idol[idol$solver == "external_solver" | idol$solver == "idol_bab",]$clean_up_threshold = NA
idol[idol$solver == "external_solver" | idol$solver == "idol_bab",]$branching_on_master = NA
```

Then, we concatenate the parameter values with the method's name to obtain a unique identifier for each tested method. The unique identifier can be `external_solver`, `idol_bab_h<A>` or `idol_bap_h<A>_s<B>_f<C>_c<D>_m<E>` where

- `<A>` is 1 if the algorithm uses primal heuristics, 0 otherwise ; 
- `<B>` is the smoothing factor (when relevant) multiplied by 100 ;
- `<C>` is 1 if Farkas pricing was used, 0 if artificial variables were used ;
- `<D>` is the clean up threshold which was used (when the threshold is reached, up to 1/3 of the threshold columns are removed from the pool) ;
- `<E>` is 1 branching was applied to the master problem and 0 otherwise.

```{r}
idol$solver_with_params = paste0(idol$solver, "_h", idol$with_heuristics, "_s", idol$smoothing_factor * 100, "_f", idol$farkas_pricing, "_c", idol$clean_up, "_m", idol$branching_on_master)
idol[idol$solver == "external_solver",]$solver_with_params = "external_solver"
idol[idol$solver == "idol_bab",]$solver_with_params = paste0("idol_bab_h", idol[idol$solver == "idol_bab",]$with_heuristics)
```

### Results obtained with coluna.jl

The results obtained using coluna can be found inside `results_GAP_coluna.csv`.

```{r}
coluna =  read.csv("../results_GAP_coluna.csv", header = FALSE)
colnames(coluna) = c("instance", "solver", "with_heuristics", "smoothing_factor", "farkas_pricing", "clean_up_threshold", "branching_on_master", "status", "reason", "n_agents", "n_jobs", "objective_value", "time")
coluna$solver_with_params = "coluna"
```

### All

We can now combine and print out the obtained computational results for each instance for each solver.

```{r}
results = rbind(idol, coluna)
```

```{r echo = FALSE}
paged_table(results)
```

The list of all tested configuration of solvers can be obtained as follows.

```{r}
solvers = as.data.frame(unique(results$solver_with_params))
colnames(solvers) = "Solver"
```

```{r echo = FALSE}
knitr::kable(solvers)
```

Additionally, we also define the used time limit for future use.

```{r}
time_limit = 5 * 60
```

## Performance profiles

To compare the different methods, we use performance profiles as introduced in [Dolan et al. (2002)](https://link.springer.com/article/10.1007/s101070100263). The following function is used to compute and plot the performance profile of a given dataset.

```{r}
performance_profile = function (dataset, color = NULL, xlim = NULL, main = "Performance profile") {
  
  solvers = unique(dataset$solver_with_params)
  times = spread(dataset[,c("instance", "solver_with_params", "time")], key = solver_with_params, value = time)
  times = na.omit(times)
  times$time.best = apply(times[,-1], 1, FUN = min)
  
  ratios = times[,-ncol(times)][,-1] / times$time.best
  colnames(ratios) = paste0(colnames(ratios), ".ratio")
  
  worst_ratio = max(ratios)
  
  times = cbind(times, ratios)
  
  for (solver in solvers) {
    time_limit_filter = times[,solver] >= time_limit
    if ( sum(time_limit_filter) > 0 ) {
      times[time_limit_filter, paste0(solver, ".ratio")] = worst_ratio
    }
  }
  
  if (is.null(xlim)) {
    xlim = c(1, worst_ratio)
  }
  
  if (is.null(color)) {
    color = rainbow(length(solvers))
  }
  
  par(mar = c(5,4,4,8))
  
  index = 1
  for (solver in solvers) {
    
    plot_function = if (index == 1) plot else lines
    
    profile = ecdf(times[,paste0(solver, ".ratio")])
    
    plot_function(profile, xlim = xlim, ylim = c(0,1), lty = "solid", cex = 0, col = color[index], main = "", xlab = "", ylab = "")
    
    index = index + 1
  }
  
  # Set the plot title
  title(main = main,
        xlab = "Performance ratio",
        ylab = "ECDF")
  
  # Set the plot legend
  legend(
    "topright",
    inset=c(-.35, 0),
    legend = solvers,
    lty = "solid",
    col = color,
    cex = .5,
    xpd = TRUE,
    bty = "n"
  )
  
}
```

### Over all methods

#### Really all

Comparing all the solvers.

```{r}
performance_profile(results, xlim = c(1, 200))
```
#### With specified settings

Comparing the external solver with branch-and-bound and branch-and-price with specific settings.

```{r}
performance_profile(results[results$solver_with_params %in% c("external_solver", "idol_bab_h1", "idol_bap_h1_s30_f0_c500_m1", "coluna"),], xlim = c(1, 200), main = "External solver, Coluna, BaB and BaP")
```

### Branch and bound

Comparing branch-and-bound with and without heuristics.

```{r}
performance_profile(results[results$solver == "idol_bab",], xlim = c(1, 3), main = "With and without heuristics")
```


### Branch and price

#### With coluna

```{r}
performance_profile(results[results$solver != "external_solver" & results$solver != "idol_bab",], xlim = c(1, 10), main = "idol versus coluna")
```

#### Branching on master versus branching on pricing

Comparing branching scheme (on master or on pricing problem) without smoothing, using artificial variables (phase I) and primal heuristics.

```{r}
performance_profile(results[results$solver == "idol_bap" & results$with_heuristics == TRUE & results$smoothing_factor == 0 & results$farkas_pricing == 0,], xlim = c(1, 3), main = "Branching on master versus subproblem")
```

#### Using artificial variables versus Farkas pricing

Comparing farkas pricing with artificial variables (phase I) using primal heuristics, master branching and no smoothing.

```{r}
performance_profile(results[results$solver == "idol_bap" & results$with_heuristics == TRUE & results$smoothing_factor == 0 & results$branching_on_master == TRUE,], xlim = c(1, 3), main = "Using artificial costs versus Farkas pricing")
```

#### Smoothing factor

Comparing smoothing factors with artificial variables, master branching and primal heuristics.

```{r}
performance_profile(results[results$solver == "idol_bap" & results$with_heuristics == TRUE & results$branching_on_master == TRUE & results$farkas_pricing == 0,], xlim = c(1, 3), main = "Smoothing factor")
```